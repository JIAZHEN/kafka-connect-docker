# The converters specify the format of data in Kafka and how to translate it into Connect data. Every Connect user will
# need to configure these based on the format they want their data in when loaded from or stored into Kafka
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
# Converter-specific settings can be passed in by prefixing the Converter's setting with the converter we want to apply
# it to
key.converter.schemas.enable=true
value.converter.schemas.enable=true

offset.storage.file.filename=/tmp/connect.offsets
# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000

name=s3-sink
connector.class=io.confluent.connect.s3.S3SinkConnector
tasks.max=1
topics=insight_json_events
flush.size=3

s3.bucket.name=broadband-deals-development
s3.region=eu-west-1
s3.part.size=5242880
s3.ssea.name=aws:kms

storage.class=io.confluent.connect.s3.storage.S3Storage
format.class=io.confluent.connect.s3.format.json.JsonFormat
schema.generator.class=io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator
schema.compatibility=NONE

key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false

partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
path.format='date'=YYYY-MM-dd/'hour'=HH
partition.duration.ms=3600000
rotate.interval.ms=60000
file.delim="-"

locale=en-GB
timezone=Europe/London
timestamp.extractor=Record

errors.tolerance=all
errors.deadletterqueue.topic.name=insight_json_events_errors
